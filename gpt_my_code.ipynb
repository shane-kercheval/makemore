{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0xffff253986b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32  # how many independent sequences will we train on in parallel?\n",
    "context_length = 8  # what is the maxiumum context length (characters) for predictions?\n",
    "num_embeddings = 32  # what is the number of embeddings for each token in our vocabulary?\n",
    "max_iterations = 3000  # how many training iterations to run in total?\n",
    "eval_interval = 300  # how many iterations to run between evaluations?\n",
    "learning_rate = 1e-2  # how quickly do we descend the gradient?\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # what device do we use to train?\n",
    "eval_iters = 200\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-17 20:28:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.9’\n",
      "\n",
      "input.txt.9         100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-12-17 20:28:06 (10.3 MB/s) - ‘input.txt.9’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(''.join(text)))) \n",
    "print(characters)\n",
    "print(len(characters))\n",
    "token_mappings = {c:i for i,c in enumerate(characters)}\n",
    "index_to_character = {i:c for i,c in enumerate(characters)}\n",
    "print(token_mappings)\n",
    "print(index_to_character)\n",
    "\n",
    "def encode(text: str) -> list[int]:\n",
    "    return [token_mappings[c] for c in text]\n",
    "\n",
    "def decode(tokens: list[int]) -> str:\n",
    "    return ''.join([index_to_character[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int (len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length+1]\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will generate 32 samples per batch\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "context_length = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "print(f\"This will generate {batch_size * context_length} samples per batch\")\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # grab e.g. 4 random starting indices\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    # construct the inputs and targets for each of the 4 sequences\n",
    "    # for each sequence, we grab the next 8 characters as the input\n",
    "    # and the 8 characters after that as the target\n",
    "    # which is why we need to subtrack the context length from the length above\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(context_length): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- self-attention explanation: https://youtu.be/kCc8FmEb1nY?si=-egoRzeqjbKo3jlf&t=3839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 7553\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 0.2163, -0.1186, -0.2929,  ..., -0.4157,  0.8209,  0.2440],\n",
      "        [ 0.3689,  0.0342, -0.3620,  ...,  0.1816,  0.7653,  0.2024],\n",
      "        [-0.0626,  0.6402, -0.2719,  ...,  0.1306,  0.2134, -0.1146],\n",
      "        ...,\n",
      "        [ 0.3244, -0.3352, -0.0098,  ..., -0.0441,  0.6653, -0.1905],\n",
      "        [-0.4548,  0.0529,  0.0245,  ..., -0.2095,  0.3074, -0.2397],\n",
      "        [-0.9060,  0.0564, -0.0106,  ..., -0.3297,  0.6187, -0.3686]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([32, 65])\n",
      "tensor(4.2648, grad_fn=<NllLossBackward0>)\n",
      "(4, 8, 65)\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int, num_embeddings: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(num_embeddings, head_size, bias=False)\n",
    "        self.query = nn.Linear(num_embeddings, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_embeddings, head_size, bias=False)\n",
    "        # a buffer is a tensor that is not a model parameter\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        # compute attention scores (i.e. 'affinities')\n",
    "        wei = q @ k.transpose(-2, -1)  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, num_embeddings: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, num_embeddings) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # each head returns a tensor of shape (B, T, head_size)\n",
    "        # we concatenate them all together and then apply a linear layer\n",
    "        # to map the concatenated tensor to the original dimension\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)  # (B, T, num_heads * head_size)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, context_length: int, num_embeddings: int):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, num_embeddings)\n",
    "        self.position_embedding_table = nn.Embedding(context_length, num_embeddings)\n",
    "        # self.self_attention_head = Head(head_size=num_embeddings, num_embeddings=num_embeddings)\n",
    "        self.self_attention_heads = MultiheadAttention(\n",
    "            num_heads=4,\n",
    "            head_size=num_embeddings // 4,\n",
    "            num_embeddings=num_embeddings,\n",
    "        )\n",
    "        self._linear_head = nn.Linear(num_embeddings, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        # B= batch, T = time (context length), C = channel (num_embeddings)\n",
    "        # e.g. if batch size is 4 and context length is 8, and num_embeddings is 56\n",
    "        # then logit.shape = (4, 8, 56)\n",
    "        token_embeddings = self.token_embedding_table(idx)  # (B, T, C) i.e. (B, T, num_embeddings)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = token_embeddings + position_embeddings  # (B, T, C)\n",
    "        x = self.self_attention_heads(x)  # apply one head of self-attention\n",
    "        logits = self._linear_head(x)  # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        # pytorch cross entropy expects (B, C, T) as input rather than (B, T, C)\n",
    "        B, T, C = logits.shape\n",
    "        # e.g. if batch size is 4 and context length is 8, and vocab size is 56\n",
    "        # then logits.view(B*T, C) will be (32, 56)\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idf is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the context is longer than the context length, then we only\n",
    "            # consider the last context_length tokens\n",
    "            index_cond = idx[:, -self.context_length:]\n",
    "            logits, _ = self(index_cond)\n",
    "            # focus only on the last time step (i.e. the last token/character)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append to the context\n",
    "            idx = torch.cat([idx, idx_next], dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "\n",
    "m = BigramLanguageModel(len(characters), context_length=context_length, num_embeddings=num_embeddings)\n",
    "m = m.to(device)\n",
    "print(f\"Model parameters: {sum([p.numel() for p in m.parameters()])}\")\n",
    "print(xb.shape)\n",
    "out, loss = m(xb, yb)\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print(loss)\n",
    "print((batch_size, context_length, len(characters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n",
      "\n",
      "ccOlyVJDq:X&edpv,b? rPDuCslAS-noNch-Nryw:$jupUj\n",
      " T'.G&GuEm !p:aljK.$u. rCpIadhkIXtRBEtnxE:cTmFXNOPN&\n"
     ]
    }
   ],
   "source": [
    "_x = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(_x)\n",
    "print(decode(m.generate(_x, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    m.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = m(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss (average): 4.190, Validation Loss (average): 4.190\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "steps = 10000\n",
    "for index in range(steps): # increase number of steps for good results... \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # keep track of the loss\n",
    "    if index % (steps / 20) == 0 or index == steps - 1:\n",
    "        average_loss = estimate_loss()\n",
    "        losses.append(average_loss)\n",
    "        print(f\"({index}) Training Loss (average): {average_loss['train']:.3f}, Validation Loss (average): {average_loss['train']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `losses` is the list of dictionaries\n",
    "train_losses = [d['train'] for d in losses]\n",
    "val_losses = [d['val'] for d in losses]\n",
    "indices = range(len(losses))\n",
    "\n",
    "plt.plot(indices, train_losses, label='Train')\n",
    "plt.plot(indices, val_losses, label='Val')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Assuming `losses` is the list of dictionaries\n",
    "train_losses = [math.log10(d['train']) for d in losses]\n",
    "val_losses = [math.log10(d['val']) for d in losses]\n",
    "indices = range(len(losses))\n",
    "\n",
    "plt.plot(indices, train_losses, label='Train')\n",
    "plt.plot(indices, val_losses, label='Val')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(context)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention mathmatical trick\n",
    "\n",
    "- we have 8 tokens in our context that are not talking to each other, and we would like them to\n",
    "- but we want to do it in a specific way e.g. token 5 should not communicate with tokens 6, 7, 8, because those tokens are in the future and token 5 doesn't know about them. But token 5 should talk to the previous tokens\n",
    "- the simplest way to do this is an average of all of the proceeding tokens; this would give the context of the previous characters \n",
    "- but we would also lose a lot of information if we just did a simple average, we would have lost the spacial (temporal?) information of all those tokens\n",
    "- but that's ok for now, we will see how we can bring the information back later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # batch size, context length (time), vocab size (channel)\n",
    "x = torch.randn(B, T, C)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is very inefficient, but it's easy to understand\n",
    "x_bag_of_words = torch.zeros(B, T, C)\n",
    "x_bag_of_words.shape\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_previous = x[b, :t+1]  # (t, C)\n",
    "        # average over the time dimension (dim=0) to get a single vector of size (C,)\n",
    "        # i think this is the average of the embeddings of the previous (and current) tokens\n",
    "        x_bag_of_words[b, t] = torch.mean(x_previous, dim=0)  # (C,)\n",
    "\n",
    "x_bag_of_words\n",
    "\n",
    "# https://youtu.be/kCc8FmEb1nY?si=nt0pZ4a7IGkQSl_E&t=2825\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of dot product we are basically doing a running average\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "print(a)\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "print(a)\n",
    "# now if we multiple by another matrix, we will get average\n",
    "# because of the dot product\n",
    "# instead of dividing by e.g. 1, 2, 3 we are multiplying by 1/1, 1/2, 1/3\n",
    "# which is the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_weights = torch.tril(torch.ones(T, T))\n",
    "print(_weights)\n",
    "_weights = _weights / torch.sum(_weights, 1, keepdim=True)\n",
    "print(_weights)\n",
    "x_bag_of_words_2 = _weights @ x\n",
    "assert torch.allclose(x_bag_of_words, x_bag_of_words_2)\n",
    "print(x_bag_of_words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3\n",
    "# https://youtu.be/kCc8FmEb1nY?si=bVuXGcd8V5ky71xL&t=3277\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "_weights = torch.zeros(T, T)\n",
    "_weights = _weights.masked_fill(tril == 0, float('-inf'))\n",
    "_weights = F.softmax(_weights, dim=1)\n",
    "x_bag_of_words_3 = _weights @ x\n",
    "assert torch.allclose(x_bag_of_words, x_bag_of_words_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
